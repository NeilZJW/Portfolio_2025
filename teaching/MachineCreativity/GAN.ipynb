{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKrItjQE4tmK",
        "outputId": "d26947cc-78a2-4443-80cd-b8ae70f20be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq \"/content/gdrive/MyDrive/Teaching/CC/ИИКб-21/13/Dataset/data.zip\""
      ],
      "metadata": {
        "id": "-v5or8PP5OL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, utils\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "m8WLMk31U89G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集加载和预处理\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # 打开图像并转换为 RGB 格式\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# 数据路径\n",
        "dataset_path = \"/content/img_align_celeba\"\n",
        "\n",
        "# 数据预处理\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # 归一化到 [-1, 1]\n",
        "])\n",
        "\n",
        "# 数据加载\n",
        "dataset = CustomDataset(root_dir=dataset_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
      ],
      "metadata": {
        "id": "6g25TwxlU6Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8 * 8 * 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(1, (128, 8, 8)),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()  # 输出范围 [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)"
      ],
      "metadata": {
        "id": "hrRtzoG8VBBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8 * 8 * 128, 1),\n",
        "            nn.Sigmoid()  # 输出范围 [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "DCu1fz2LVD5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 初始化模型\n",
        "latent_dim = 100\n",
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss()  # 二元交叉熵\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
      ],
      "metadata": {
        "id": "k0P_1HS2VM4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qdyLyjiaVLRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlJqq-7uaGMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af09707a-ec17-45d7-b4b3-adfc648d1b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Batch 0/1583 Loss D: 1.3897, Loss G: 0.9324\n",
            "Epoch [1/50] Batch 100/1583 Loss D: 0.0368, Loss G: 5.5936\n",
            "Epoch [1/50] Batch 200/1583 Loss D: 0.1660, Loss G: 4.9337\n",
            "Epoch [1/50] Batch 300/1583 Loss D: 0.5893, Loss G: 1.3278\n",
            "Epoch [1/50] Batch 400/1583 Loss D: 0.2461, Loss G: 3.4950\n",
            "Epoch [1/50] Batch 500/1583 Loss D: 0.2640, Loss G: 3.0103\n",
            "Epoch [1/50] Batch 600/1583 Loss D: 0.2076, Loss G: 2.9829\n",
            "Epoch [1/50] Batch 700/1583 Loss D: 0.3458, Loss G: 2.8806\n",
            "Epoch [1/50] Batch 800/1583 Loss D: 0.3739, Loss G: 2.2323\n",
            "Epoch [1/50] Batch 900/1583 Loss D: 0.3107, Loss G: 3.1705\n",
            "Epoch [1/50] Batch 1000/1583 Loss D: 0.2178, Loss G: 3.4153\n",
            "Epoch [1/50] Batch 1100/1583 Loss D: 0.2402, Loss G: 3.3543\n",
            "Epoch [1/50] Batch 1200/1583 Loss D: 0.8169, Loss G: 1.3823\n",
            "Epoch [1/50] Batch 1300/1583 Loss D: 0.2914, Loss G: 3.0399\n",
            "Epoch [1/50] Batch 1400/1583 Loss D: 0.3812, Loss G: 1.9732\n",
            "Epoch [1/50] Batch 1500/1583 Loss D: 0.2559, Loss G: 2.8487\n",
            "Epoch [2/50] Batch 0/1583 Loss D: 0.3616, Loss G: 2.6236\n",
            "Epoch [2/50] Batch 100/1583 Loss D: 0.3215, Loss G: 2.5374\n",
            "Epoch [2/50] Batch 200/1583 Loss D: 0.3462, Loss G: 1.4936\n",
            "Epoch [2/50] Batch 300/1583 Loss D: 2.2242, Loss G: 3.9866\n",
            "Epoch [2/50] Batch 400/1583 Loss D: 0.1940, Loss G: 3.0603\n",
            "Epoch [2/50] Batch 500/1583 Loss D: 0.4243, Loss G: 2.9958\n",
            "Epoch [2/50] Batch 600/1583 Loss D: 0.2724, Loss G: 3.0270\n",
            "Epoch [2/50] Batch 700/1583 Loss D: 0.6045, Loss G: 2.4092\n",
            "Epoch [2/50] Batch 800/1583 Loss D: 0.2883, Loss G: 2.3598\n",
            "Epoch [2/50] Batch 900/1583 Loss D: 0.2702, Loss G: 3.6254\n",
            "Epoch [2/50] Batch 1000/1583 Loss D: 0.6056, Loss G: 4.5558\n",
            "Epoch [2/50] Batch 1100/1583 Loss D: 0.2036, Loss G: 2.9673\n",
            "Epoch [2/50] Batch 1200/1583 Loss D: 0.3164, Loss G: 2.3036\n",
            "Epoch [2/50] Batch 1300/1583 Loss D: 0.2082, Loss G: 2.9501\n",
            "Epoch [2/50] Batch 1400/1583 Loss D: 0.2543, Loss G: 2.6848\n",
            "Epoch [2/50] Batch 1500/1583 Loss D: 0.1882, Loss G: 3.0163\n",
            "Epoch [3/50] Batch 0/1583 Loss D: 2.1405, Loss G: 0.5788\n",
            "Epoch [3/50] Batch 100/1583 Loss D: 0.3447, Loss G: 1.8325\n",
            "Epoch [3/50] Batch 200/1583 Loss D: 0.1860, Loss G: 3.3274\n",
            "Epoch [3/50] Batch 300/1583 Loss D: 0.1544, Loss G: 2.6743\n",
            "Epoch [3/50] Batch 400/1583 Loss D: 0.2209, Loss G: 3.1330\n",
            "Epoch [3/50] Batch 500/1583 Loss D: 0.1765, Loss G: 3.2270\n",
            "Epoch [3/50] Batch 600/1583 Loss D: 0.1979, Loss G: 3.1756\n",
            "Epoch [3/50] Batch 700/1583 Loss D: 0.1889, Loss G: 3.0631\n",
            "Epoch [3/50] Batch 800/1583 Loss D: 0.1922, Loss G: 2.6670\n",
            "Epoch [3/50] Batch 900/1583 Loss D: 0.1820, Loss G: 3.1303\n",
            "Epoch [3/50] Batch 1000/1583 Loss D: 0.3813, Loss G: 3.1174\n",
            "Epoch [3/50] Batch 1100/1583 Loss D: 0.2423, Loss G: 2.3768\n",
            "Epoch [3/50] Batch 1200/1583 Loss D: 0.1810, Loss G: 2.8777\n",
            "Epoch [3/50] Batch 1300/1583 Loss D: 0.1916, Loss G: 3.4635\n",
            "Epoch [3/50] Batch 1400/1583 Loss D: 0.4145, Loss G: 3.8161\n",
            "Epoch [3/50] Batch 1500/1583 Loss D: 0.2107, Loss G: 2.3441\n",
            "Epoch [4/50] Batch 0/1583 Loss D: 0.2251, Loss G: 2.8264\n",
            "Epoch [4/50] Batch 100/1583 Loss D: 0.2583, Loss G: 3.8479\n",
            "Epoch [4/50] Batch 200/1583 Loss D: 0.2279, Loss G: 3.0418\n",
            "Epoch [4/50] Batch 300/1583 Loss D: 0.2174, Loss G: 2.9187\n",
            "Epoch [4/50] Batch 400/1583 Loss D: 0.1288, Loss G: 3.2454\n",
            "Epoch [4/50] Batch 500/1583 Loss D: 0.2473, Loss G: 3.4301\n",
            "Epoch [4/50] Batch 600/1583 Loss D: 0.1356, Loss G: 2.9205\n",
            "Epoch [4/50] Batch 700/1583 Loss D: 0.2326, Loss G: 2.7669\n",
            "Epoch [4/50] Batch 800/1583 Loss D: 0.1653, Loss G: 3.5779\n",
            "Epoch [4/50] Batch 900/1583 Loss D: 0.1593, Loss G: 2.6165\n",
            "Epoch [4/50] Batch 1000/1583 Loss D: 0.2077, Loss G: 3.1784\n",
            "Epoch [4/50] Batch 1100/1583 Loss D: 0.2811, Loss G: 1.5589\n",
            "Epoch [4/50] Batch 1200/1583 Loss D: 0.1609, Loss G: 3.3922\n",
            "Epoch [4/50] Batch 1300/1583 Loss D: 0.1430, Loss G: 3.0606\n",
            "Epoch [4/50] Batch 1400/1583 Loss D: 0.1733, Loss G: 3.3025\n",
            "Epoch [4/50] Batch 1500/1583 Loss D: 0.1349, Loss G: 3.4852\n",
            "Epoch [5/50] Batch 0/1583 Loss D: 0.1827, Loss G: 2.5327\n",
            "Epoch [5/50] Batch 100/1583 Loss D: 0.6533, Loss G: 1.6256\n",
            "Epoch [5/50] Batch 200/1583 Loss D: 0.1606, Loss G: 3.4979\n",
            "Epoch [5/50] Batch 300/1583 Loss D: 0.1715, Loss G: 3.1600\n",
            "Epoch [5/50] Batch 400/1583 Loss D: 0.5124, Loss G: 5.7416\n",
            "Epoch [5/50] Batch 500/1583 Loss D: 0.6506, Loss G: 2.4735\n",
            "Epoch [5/50] Batch 600/1583 Loss D: 0.1210, Loss G: 3.4409\n",
            "Epoch [5/50] Batch 700/1583 Loss D: 0.1068, Loss G: 3.7088\n",
            "Epoch [5/50] Batch 800/1583 Loss D: 0.2023, Loss G: 4.3725\n",
            "Epoch [5/50] Batch 900/1583 Loss D: 0.1303, Loss G: 3.0179\n",
            "Epoch [5/50] Batch 1000/1583 Loss D: 0.1562, Loss G: 3.7353\n",
            "Epoch [5/50] Batch 1100/1583 Loss D: 0.1573, Loss G: 3.1530\n",
            "Epoch [5/50] Batch 1200/1583 Loss D: 0.2744, Loss G: 2.1407\n",
            "Epoch [5/50] Batch 1300/1583 Loss D: 1.0071, Loss G: 1.4666\n",
            "Epoch [5/50] Batch 1400/1583 Loss D: 0.1481, Loss G: 3.1439\n",
            "Epoch [5/50] Batch 1500/1583 Loss D: 0.2379, Loss G: 2.0448\n",
            "Epoch [6/50] Batch 0/1583 Loss D: 0.1595, Loss G: 3.4065\n",
            "Epoch [6/50] Batch 100/1583 Loss D: 0.1330, Loss G: 3.4599\n",
            "Epoch [6/50] Batch 200/1583 Loss D: 0.4022, Loss G: 5.7742\n",
            "Epoch [6/50] Batch 300/1583 Loss D: 0.1894, Loss G: 2.6819\n",
            "Epoch [6/50] Batch 400/1583 Loss D: 0.1165, Loss G: 4.0025\n",
            "Epoch [6/50] Batch 500/1583 Loss D: 0.1176, Loss G: 4.1605\n",
            "Epoch [6/50] Batch 600/1583 Loss D: 0.1835, Loss G: 3.2067\n",
            "Epoch [6/50] Batch 700/1583 Loss D: 0.1654, Loss G: 3.4556\n",
            "Epoch [6/50] Batch 800/1583 Loss D: 0.1327, Loss G: 3.4288\n",
            "Epoch [6/50] Batch 900/1583 Loss D: 0.1029, Loss G: 4.0340\n",
            "Epoch [6/50] Batch 1000/1583 Loss D: 0.1112, Loss G: 4.4649\n",
            "Epoch [6/50] Batch 1100/1583 Loss D: 0.1864, Loss G: 3.0766\n",
            "Epoch [6/50] Batch 1200/1583 Loss D: 0.3707, Loss G: 5.4466\n",
            "Epoch [6/50] Batch 1300/1583 Loss D: 0.0954, Loss G: 3.9206\n",
            "Epoch [6/50] Batch 1400/1583 Loss D: 0.1648, Loss G: 3.8391\n",
            "Epoch [6/50] Batch 1500/1583 Loss D: 0.1323, Loss G: 3.3319\n",
            "Epoch [7/50] Batch 0/1583 Loss D: 0.6929, Loss G: 4.1567\n",
            "Epoch [7/50] Batch 100/1583 Loss D: 0.1406, Loss G: 3.4272\n",
            "Epoch [7/50] Batch 200/1583 Loss D: 0.1058, Loss G: 3.7939\n",
            "Epoch [7/50] Batch 300/1583 Loss D: 0.1796, Loss G: 2.6665\n",
            "Epoch [7/50] Batch 400/1583 Loss D: 0.8054, Loss G: 1.7358\n",
            "Epoch [7/50] Batch 500/1583 Loss D: 0.1761, Loss G: 3.3732\n",
            "Epoch [7/50] Batch 600/1583 Loss D: 0.1399, Loss G: 3.6959\n",
            "Epoch [7/50] Batch 700/1583 Loss D: 0.3255, Loss G: 4.7618\n",
            "Epoch [7/50] Batch 800/1583 Loss D: 0.1821, Loss G: 3.0008\n",
            "Epoch [7/50] Batch 900/1583 Loss D: 0.1114, Loss G: 3.6181\n",
            "Epoch [7/50] Batch 1000/1583 Loss D: 0.1263, Loss G: 3.3938\n",
            "Epoch [7/50] Batch 1100/1583 Loss D: 0.1191, Loss G: 3.9600\n",
            "Epoch [7/50] Batch 1200/1583 Loss D: 1.1782, Loss G: 4.3822\n",
            "Epoch [7/50] Batch 1300/1583 Loss D: 0.1956, Loss G: 2.8891\n",
            "Epoch [7/50] Batch 1400/1583 Loss D: 0.1305, Loss G: 3.3564\n",
            "Epoch [7/50] Batch 1500/1583 Loss D: 0.1589, Loss G: 3.5162\n",
            "Epoch [8/50] Batch 0/1583 Loss D: 0.1378, Loss G: 2.8938\n",
            "Epoch [8/50] Batch 100/1583 Loss D: 0.4691, Loss G: 2.9560\n",
            "Epoch [8/50] Batch 200/1583 Loss D: 0.1762, Loss G: 3.2206\n",
            "Epoch [8/50] Batch 300/1583 Loss D: 0.1438, Loss G: 3.2940\n",
            "Epoch [8/50] Batch 400/1583 Loss D: 0.1220, Loss G: 3.5408\n",
            "Epoch [8/50] Batch 500/1583 Loss D: 0.1563, Loss G: 3.1858\n",
            "Epoch [8/50] Batch 600/1583 Loss D: 0.1200, Loss G: 3.1213\n",
            "Epoch [8/50] Batch 700/1583 Loss D: 0.8719, Loss G: 5.9057\n",
            "Epoch [8/50] Batch 800/1583 Loss D: 0.1978, Loss G: 2.5749\n",
            "Epoch [8/50] Batch 900/1583 Loss D: 4.7619, Loss G: 14.3448\n",
            "Epoch [8/50] Batch 1000/1583 Loss D: 0.1104, Loss G: 3.5343\n",
            "Epoch [8/50] Batch 1100/1583 Loss D: 0.2079, Loss G: 3.5737\n",
            "Epoch [8/50] Batch 1200/1583 Loss D: 0.1235, Loss G: 3.5456\n",
            "Epoch [8/50] Batch 1300/1583 Loss D: 0.1319, Loss G: 3.6277\n",
            "Epoch [8/50] Batch 1400/1583 Loss D: 0.1236, Loss G: 3.7256\n",
            "Epoch [8/50] Batch 1500/1583 Loss D: 0.0973, Loss G: 4.0834\n",
            "Epoch [9/50] Batch 0/1583 Loss D: 0.7832, Loss G: 2.2535\n",
            "Epoch [9/50] Batch 100/1583 Loss D: 0.1705, Loss G: 3.0602\n",
            "Epoch [9/50] Batch 200/1583 Loss D: 0.0871, Loss G: 4.0192\n",
            "Epoch [9/50] Batch 300/1583 Loss D: 0.0804, Loss G: 4.2166\n",
            "Epoch [9/50] Batch 400/1583 Loss D: 0.1307, Loss G: 3.8390\n",
            "Epoch [9/50] Batch 500/1583 Loss D: 0.2761, Loss G: 2.6310\n",
            "Epoch [9/50] Batch 600/1583 Loss D: 0.1622, Loss G: 3.2901\n",
            "Epoch [9/50] Batch 700/1583 Loss D: 0.2298, Loss G: 4.5610\n",
            "Epoch [9/50] Batch 800/1583 Loss D: 0.1016, Loss G: 4.2204\n",
            "Epoch [9/50] Batch 900/1583 Loss D: 0.1282, Loss G: 3.3619\n",
            "Epoch [9/50] Batch 1000/1583 Loss D: 0.1027, Loss G: 4.3901\n",
            "Epoch [9/50] Batch 1100/1583 Loss D: 0.1718, Loss G: 3.4201\n",
            "Epoch [9/50] Batch 1200/1583 Loss D: 0.1381, Loss G: 3.8234\n",
            "Epoch [9/50] Batch 1300/1583 Loss D: 0.1606, Loss G: 3.3892\n",
            "Epoch [9/50] Batch 1400/1583 Loss D: 0.1469, Loss G: 3.8188\n",
            "Epoch [9/50] Batch 1500/1583 Loss D: 0.1355, Loss G: 3.1995\n",
            "Epoch [10/50] Batch 0/1583 Loss D: 0.1165, Loss G: 3.6330\n",
            "Epoch [10/50] Batch 100/1583 Loss D: 0.1235, Loss G: 3.5247\n",
            "Epoch [10/50] Batch 200/1583 Loss D: 0.1050, Loss G: 3.5087\n",
            "Epoch [10/50] Batch 300/1583 Loss D: 4.2687, Loss G: 0.0007\n",
            "Epoch [10/50] Batch 400/1583 Loss D: 0.1595, Loss G: 3.5579\n",
            "Epoch [10/50] Batch 500/1583 Loss D: 0.3187, Loss G: 3.2874\n",
            "Epoch [10/50] Batch 600/1583 Loss D: 0.1262, Loss G: 3.6118\n",
            "Epoch [10/50] Batch 700/1583 Loss D: 0.1297, Loss G: 4.0334\n",
            "Epoch [10/50] Batch 800/1583 Loss D: 0.5236, Loss G: 1.6622\n",
            "Epoch [10/50] Batch 900/1583 Loss D: 0.1927, Loss G: 3.2373\n",
            "Epoch [10/50] Batch 1000/1583 Loss D: 0.1532, Loss G: 3.9130\n",
            "Epoch [10/50] Batch 1100/1583 Loss D: 0.0752, Loss G: 4.2602\n",
            "Epoch [10/50] Batch 1200/1583 Loss D: 0.4700, Loss G: 2.2189\n",
            "Epoch [10/50] Batch 1300/1583 Loss D: 0.1036, Loss G: 4.8400\n",
            "Epoch [10/50] Batch 1400/1583 Loss D: 0.0659, Loss G: 4.2165\n",
            "Epoch [10/50] Batch 1500/1583 Loss D: 0.1223, Loss G: 4.3453\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 训练循环\n",
        "epochs = 50\n",
        "fixed_noise = torch.randn(16, latent_dim).to(device)  # 用于保存固定噪声生成的图像\n",
        "\n",
        "if not os.path.exists('generated_images'):\n",
        "    os.makedirs('generated_images')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, real_images in enumerate(dataloader):\n",
        "        real_images = real_images.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # 训练判别器\n",
        "        optimizer_D.zero_grad()\n",
        "        real_labels = torch.ones((batch_size, 1)).to(device)  # 真实标签\n",
        "        fake_labels = torch.zeros((batch_size, 1)).to(device)  # 假标签\n",
        "\n",
        "        # 判别器对真实图像的损失\n",
        "        outputs = discriminator(real_images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "        # 判别器对假图像的损失\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)  # 随机噪声\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images.detach())  # 假图像不回传生成器梯度\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "        # 总判别器损失\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # 训练生成器\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_labels = torch.ones((batch_size, 1)).to(device)  # 生成器希望让判别器认为假的图像为真\n",
        "        outputs = discriminator(fake_images)\n",
        "        g_loss = criterion(outputs, fake_labels)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] Batch {i}/{len(dataloader)} \"\n",
        "                  f\"Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\")\n",
        "\n",
        "    # 保存生成图像\n",
        "    with torch.no_grad():\n",
        "        fake_images = generator(fixed_noise).detach().cpu()\n",
        "        fake_images = (fake_images + 1) / 2.0  # 将范围从 [-1, 1] 转换到 [0, 1]\n",
        "        grid = utils.make_grid(fake_images, nrow=4)\n",
        "        utils.save_image(grid, f\"generated_images/epoch_{epoch+1}.png\")\n",
        "\n",
        "print(\"Training finished. Generated images are saved in 'generated_images' folder.\")\n"
      ]
    }
  ]
}